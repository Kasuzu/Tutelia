 # === LLM local vía LM Studio ===
 LLM_MODEL=google/gemma-3n-e4b
 OPENAI_API_BASE=http://127.0.0.1:1234/v1
 OPENAI_API_KEY=lm-studio
-LLM_TEMPERATURE=0.2
+LLM_TEMPERATURE=0.25
+TOP_P=0.9
+REPETITION_PENALTY=1.05

 # === Salida del modelo ===
-# Máximo de tokens que el backend intentará pedir por turno
-MAX_TOKENS_STEP=1024
+# Máximo de tokens que el backend intentará pedir por turno (prompt+output cuentan)
+MAX_TOKENS_STEP=1500
 # (Opcional, por si otro módulo lo usa)
 LLM_MAX_TOKENS=4096


# === Embeddings locales (HuggingFace) ===
EMBEDDING_MODEL=intfloat/multilingual-e5-small

# === RAG (retrieval) ===
TOP_K=3
RAG_TOP_K=3  # (solo si otro código lo usa; app.py lee TOP_K)

# === Vector DB (Chroma) ===
PERSIST_DIR=./chroma

# === Ingesta de documentos ===
DOCS_DIR=./docs
CHUNK_SIZE=600
CHUNK_OVERLAP=120

# === Comportamiento del asesor ===
# Con esto, si no hay documentos relevantes en el índice, no contesta.
STRICT_CONTEXT=1

# === Exportaciones (PDF/TeX del wizard) ===
EXPORT_DIR=./exports
